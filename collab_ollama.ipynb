{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tMgUKTy9kC-",
        "outputId": "72812cfe-2edd-49f3-8127-8b62aa0c79a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0>>> Downloading ollama...\n",
            "100  8575    0  8575    0     0  38234      0 --:--:-- --:--:-- --:--:-- 38281\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA GPU. Install lspci or lshw to automatically detect and install NVIDIA CUDA drivers.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl https://ollama.ai/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3L8K01L-W2x",
        "outputId": "b80590e2-2b80-48e7-83f8-c7c6b9860375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.5-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTMZBxfH-eT9",
        "outputId": "d99c3d48-c134-4d36-a6f9-39c1ce29cb72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "import threading\n",
        "import time\n",
        "import os\n",
        "import asyncio\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import queue\n",
        "import time\n",
        "from threading import Thread\n",
        "\n",
        "# Get your ngrok token from your ngrok account:\n",
        "# https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "token=\"2cXIV09cJzKkS3NcqZ1XiUcyD7P_7n6grFj2ao1TM1JWmxrqg\"\n",
        "ngrok.set_auth_token(token)\n",
        "\n",
        "# set up a stoppable thread (not mandatory, but cleaner if you want to stop this later\n",
        "class StoppableThread(threading.Thread):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(StoppableThread, self).__init__(*args, **kwargs)\n",
        "        self._stop_event = threading.Event()\n",
        "\n",
        "    def stop(self):\n",
        "        self._stop_event.set()\n",
        "\n",
        "    def is_stopped(self):\n",
        "        return self._stop_event.is_set()\n",
        "\n",
        "def start_ngrok(q, stop_event):\n",
        "    try:\n",
        "        # Start an HTTP tunnel on the specified port\n",
        "        public_url = ngrok.connect(11434)\n",
        "        # Put the public URL in the queue\n",
        "        q.put(public_url)\n",
        "        # Keep the thread alive until stop event is set\n",
        "        while not stop_event.is_set():\n",
        "            time.sleep(1)  # Adjust sleep time as needed\n",
        "    except Exception as e:\n",
        "        print(f\"Error in start_ngrok: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x3J4NcNm_IJ-"
      },
      "outputs": [],
      "source": [
        "# Create a queue to share data between threads\n",
        "url_queue = queue.Queue()\n",
        "\n",
        "# Start ngrok in a separate thread\n",
        "ngrok_thread = StoppableThread(target=start_ngrok, args=(url_queue, StoppableThread.is_stopped))\n",
        "ngrok_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baaT9ZOT_TuM",
        "outputId": "e5bf73dd-8f2b-419a-fb89-12098050fe31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ngrok tunnel established at: NgrokTunnel: \"https://45ec-34-125-114-25.ngrok-free.app\" -> \"http://localhost:11434\"\n"
          ]
        }
      ],
      "source": [
        "# Wait for the ngrok tunnel to be established\n",
        "while True:\n",
        "    try:\n",
        "        public_url = url_queue.get()\n",
        "        if public_url:\n",
        "            break\n",
        "        print(\"Waiting for ngrok URL...\")\n",
        "        time.sleep(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in retrieving ngrok URL: {e}\")\n",
        "\n",
        "print(\"Ngrok tunnel established at:\", public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rdmNc8-y_X-n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "\n",
        "# NB: You may need to set these depending and get cuda working depending which backend you are running.\n",
        "# Set environment variable for NVIDIA library\n",
        "# Set environment variables for CUDA\n",
        "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
        "# Set LD_LIBRARY_PATH to include both /usr/lib64-nvidia and CUDA lib directories\n",
        "os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'\n",
        "\n",
        "async def run_process(cmd):\n",
        "    print('>>> starting', *cmd)\n",
        "    process = await asyncio.create_subprocess_exec(\n",
        "        *cmd,\n",
        "        stdout=asyncio.subprocess.PIPE,\n",
        "        stderr=asyncio.subprocess.PIPE\n",
        "    )\n",
        "\n",
        "    # define an async pipe function\n",
        "    async def pipe(lines):\n",
        "        async for line in lines:\n",
        "            print(line.decode().strip())\n",
        "\n",
        "        await asyncio.gather(\n",
        "            pipe(process.stdout),\n",
        "            pipe(process.stderr),\n",
        "        )\n",
        "\n",
        "    # call it\n",
        "    await asyncio.gather(pipe(process.stdout), pipe(process.stderr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQMmd2Wp_j58",
        "outputId": "bcf97c48-f886-460d-fe54-05214b0a982c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> starting ollama serve\n",
            "Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n",
            "Your new public key is:\n",
            "\n",
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFzoCWT1Mh0yS8lqkTVZYQWodIf9VfA+VvcvYmBXM4PQ\n",
            "\n",
            "time=2024-03-14T12:44:57.728Z level=INFO source=images.go:710 msg=\"total blobs: 0\"\n",
            "time=2024-03-14T12:44:57.728Z level=INFO source=images.go:717 msg=\"total unused blobs removed: 0\"\n",
            "time=2024-03-14T12:44:57.729Z level=INFO source=routes.go:1021 msg=\"Listening on 127.0.0.1:11434 (version 0.1.28)\"\n",
            "time=2024-03-14T12:44:57.730Z level=INFO source=payload_common.go:107 msg=\"Extracting dynamic libraries...\"\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import threading\n",
        "\n",
        "async def start_ollama_serve():\n",
        "    await run_process(['ollama', 'serve'])\n",
        "\n",
        "def run_async_in_thread(loop, coro):\n",
        "    asyncio.set_event_loop(loop)\n",
        "    loop.run_until_complete(coro)\n",
        "    loop.close()\n",
        "\n",
        "# Create a new event loop that will run in a new thread\n",
        "new_loop = asyncio.new_event_loop()\n",
        "\n",
        "# Start ollama serve in a separate thread so the cell won't block execution\n",
        "thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n",
        "thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMyGxdxaBvwr"
      },
      "source": [
        "run following line in terminal to use collab ollama on local.\n",
        "export OLLAMA_HOST=ngrok link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t49qVcZ3AAxg"
      },
      "outputs": [],
      "source": [
        "# !curl https://ollama.ai/install.sh | sh\n",
        "!curl -fsSL https://ollama.com/install.sh | sed 's#https://ollama.com/download#https://github.com/jmorganca/ollama/releases/download/v0.1.28#' | sh\n",
        "\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
        "\n",
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token('2cXIV09cJzKkS3NcqZ1XiUcyD7P_7n6grFj2ao1TM1JWmxrqg')\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "async def run_process(cmd):\n",
        "  print('>>> starting', *cmd)\n",
        "  p = await asyncio.subprocess.create_subprocess_exec(\n",
        "      *cmd,\n",
        "      stdout=asyncio.subprocess.PIPE,\n",
        "      stderr=asyncio.subprocess.PIPE,\n",
        "  )\n",
        "\n",
        "  async def pipe(lines):\n",
        "    async for line in lines:\n",
        "      print(line.strip().decode('utf-8'))\n",
        "\n",
        "  await asyncio.gather(\n",
        "      pipe(p.stdout),\n",
        "      pipe(p.stderr),\n",
        "  )\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "await asyncio.gather(\n",
        "    run_process(['ollama', 'serve']),\n",
        "    run_process(['ngrok', 'http', '--log', 'stderr', '--domain','in-shrimp-lightly.ngrok-free.app','11434']),\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
